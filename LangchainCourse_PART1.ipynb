{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqrd4F/k7IjFRnYM1mI/rL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanviredu/CompleteDataAnalystCourse/blob/main/LangchainCourse_PART1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0cP6SvWgRbXp"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "from google.colab import output\n",
        "!apt-get update && apt-get install -y curl\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!ollama serve > /dev/null 2>&1 &\n",
        "!curl http://localhost:11434/api/version\n",
        "!ollama pull llama2\n",
        "from openai import OpenAI\n",
        "MODEL = \"llama2\"\n",
        "openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
        "response = openai.chat.completions.create(\n",
        " model=MODEL,\n",
        " messages=[{\"role\": \"user\", \"content\": \"2+2 = ?\"}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "import time\n",
        "time.sleep(3)\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tanviredu/Mastering-LangChain/refs/heads/main/0-Data/paul_graham_essay3.txt\n",
        "!wget https://raw.githubusercontent.com/tanviredu/Mastering-LangChain/refs/heads/main/0-Data/paul_graham_short.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVnPy27GVzo2",
        "outputId": "15a6e188-add0-4922-ea94-a16a93d6441e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-11 06:03:33--  https://raw.githubusercontent.com/tanviredu/Mastering-LangChain/refs/heads/main/0-Data/paul_graham_essay3.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‚Äòpaul_graham_essay3.txt‚Äô\n",
            "\n",
            "paul_graham_essay3. 100%[===================>]  73.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-02-11 06:03:33 (3.52 MB/s) - ‚Äòpaul_graham_essay3.txt‚Äô saved [75042/75042]\n",
            "\n",
            "--2025-02-11 06:03:33--  https://raw.githubusercontent.com/tanviredu/Mastering-LangChain/refs/heads/main/0-Data/paul_graham_short.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 161 [text/plain]\n",
            "Saving to: ‚Äòpaul_graham_short.txt‚Äô\n",
            "\n",
            "paul_graham_short.t 100%[===================>]     161  --.-KB/s    in 0s      \n",
            "\n",
            "2025-02-11 06:03:33 (8.23 MB/s) - ‚Äòpaul_graham_short.txt‚Äô saved [161/161]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install faiss-cpu\n",
        "!pip install langchain-openai\n",
        "\n",
        "print(\"Installed Successfully\")\n",
        "time.sleep(4)\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "6j2MOMbEXoa6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter        import CharacterTextSplitter\n",
        "from langchain.document_loaders     import TextLoader\n",
        "from langchain_openai               import OpenAIEmbeddings,ChatOpenAI\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain.vectorstores         import FAISS\n",
        "from langchain.chains               import RetrievalQA\n",
        "from langchain.schema import Document"
      ],
      "metadata": {
        "id": "VGHGE6ZdYIrg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"./paul_graham_short.txt\"\n",
        "loader = TextLoader(file_path)\n",
        "documents = loader.load()\n",
        "print(documents[0])\n",
        "time.sleep(8)\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "tAG9J6pwaIDe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=3)\n",
        "splits_docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "UonRu-i6aOZb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = OllamaEmbeddings(model=\"llama2\")"
      ],
      "metadata": {
        "id": "0dkojZWdbWfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45f146e7-7b10-4087-e674-5fd0a68f4950"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-9de4dbba1323>:1: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
            "  embedding = OllamaEmbeddings(model=\"llama2\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectore_store =FAISS.from_documents(splits_docs,embedding)"
      ],
      "metadata": {
        "id": "qCWKX4u1cJfr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "# Use FAISS as a retriever\n",
        "retriever = vectore_store.as_retriever()\n",
        "\n",
        "# Set up a Retrieval-based QA system using LLaMA\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=Ollama(model=\"llama2\"),  # Change model to \"llama3\" or \"mistral\" if needed\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "pbGVcUvaefLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff27b01-e8af-42a1-c6e7-7ac895b4e150"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-b1ad7ddb42dc>:7: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
            "  llm=Ollama(model=\"llama2\"),  # Change model to \"llama3\" or \"mistral\" if needed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"What you want to know\")\n",
        "result = qa_chain.invoke({\"query\":query})\n",
        "print(\"Answer: \")\n",
        "print(result['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "B0xiFnFtog9b",
        "outputId": "1f362353-ee1a-4676-f8da-63ed406bc7fc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What you want to knowwhere tanvir study?\n",
            "Answer: \n",
            "{'query': 'where tanvir study?', 'result': 'Sure, I can answer your question! Based on the context you provided, Tanvir Rahman studied at Chittagong University of Engineering and Technology (CUET).', 'source_documents': [Document(id='b079659a-c6a0-40c2-acc2-c687fbd6702c', metadata={'source': './paul_graham_short.txt'}, page_content='My Name is Tanvir Rahman. My age is 30. I studied in Chittagong University of Engineering and Technology. In short form it is called CUET. I live in Chittagong.')]}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-16a3fb0d2737>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What you want to know\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "‚ùì Does RetrievalQA Modify the LLaMA Model with New Data?\n",
        "No, it does not update the LLaMA model itself. Instead, it retrieves relevant documents from FAISS and uses them as context for generating answers.\n",
        "\n",
        "üîç What Happens When You Use RetrievalQA?\n",
        "1Ô∏è‚É£ LLaMA stays the same ‚Äì It is not fine-tuned or retrained.\n",
        "2Ô∏è‚É£ New data is stored in FAISS, not in the model.\n",
        "3Ô∏è‚É£ At query time, the retriever fetches relevant chunks from FAISS and feeds them to LLaMA.\n",
        "4Ô∏è‚É£ LLaMA generates answers based on retrieved documents instead of relying only on its pre-trained knowledge.\n",
        "\n",
        "üìù Example: How It Works\n",
        "Imagine you have a document saying:\n",
        "\n",
        "\"Elon Musk founded SpaceX in 2002.\"\n",
        "\n",
        "If you ask GPT-based LLaMA (without FAISS):\n",
        "‚ùå \"When was SpaceX founded?\"\n",
        "It might hallucinate (make up an answer) if it lacks this knowledge.\n",
        "\n",
        "But if you store the document in FAISS and ask the same question using RetrievalQA:\n",
        "‚úÖ FAISS retrieves the text chunk, and LLaMA uses it to generate a more accurate answer.\n",
        "\n",
        "üîπ Key Takeaway\n",
        "LLaMA model remains unchanged ‚Äì it is not retrained or updated.\n",
        "New knowledge is added to FAISS, not LLaMA.\n",
        "RetrievalQA enables real-time document-based Q&A without modifying the model.\n",
        "üöÄ Think of it like an external \"memory\" for LLaMA!\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "-ZJGDX32tuKS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jgikjXsbo8ej"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}